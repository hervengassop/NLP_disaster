{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import random as rd\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.util import minibatch, compounding\n",
    "from nltk.stem.porter import *  \n",
    "\n",
    "import re\n",
    "import urllib.request\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_columns = None\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "from spacy import displacy\n",
    "import df_helper as dfh\n",
    "import nlp_support as ns\n",
    "\n",
    "\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "import importlib\n",
    "import pickle\n",
    "from sklearn.feature_selection import SelectKBest, chi2,  RFE, RFECV, SelectFpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<module 'nlp_support' from '/home/marco/UD/NLP_disaster/nlp_support.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dfh)\n",
    "importlib.reload(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, validation_curve\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, fbeta_score, classification_report, confusion_matrix\n",
    "\n",
    "import sklearn.metrics as met\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, make_scorer, average_precision_score, auc, \\\n",
    "    accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, classification_report, \\\n",
    "    brier_score_loss, roc_auc_score\n",
    "\n",
    "from scipy.stats import randint as sp_randint , uniform\n",
    "import eli5\n",
    "\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/MLE/ml/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3248: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under_false_2500_nwords_100_mindf_1\n",
      "Original n_true, n_false: 2329 18717\n",
      "Resampled n_true, n_false: 2329 2500\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n1 -r1\n",
    "\n",
    "if 1:   \n",
    "    \n",
    "    X_train,Y_train,X_test, Y_test,X_valid, Y_valid= ns.load_data()\n",
    "    n_false_sample= 2500\n",
    "    n_features=100\n",
    "\n",
    "    DOE='under_false_%s_nwords_%s_mindf_1' % (n_false_sample,n_features)\n",
    "    print(DOE)\n",
    "\n",
    "    data_set, bow_bal, tfidf = ns.train_bow(X_train,Y_train, X_test,Y_test,n_false_sample ,  max_df=0.9,min_df=1, max_features=n_features )\n",
    "\n",
    "    with open('%s_data.pkl' % DOE, 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(data_set, f)\n",
    "        \n",
    "        from joblib import dump, load\n",
    "        dump(tfidf, '%s_tfidf.joblib' % DOE)\n",
    "        dump(bow_bal, '%s_bal_cv.joblib' % DOE)\n",
    "        \n",
    "    print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_bal=set(bow_bal.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DOE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-526ba73e83e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s_data.pkl'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mDOE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Pickle the 'data' dictionary using the highest protocol available.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DOE' is not defined"
     ]
    }
   ],
   "source": [
    "# with open('%s_data.pkl' % DOE, 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(data_set, f)\n",
    "\n",
    "#     from joblib import dump, load\n",
    "#     dump(tfidf, '%s_tfidf.joblib' % DOE)\n",
    "#     dump(bow_bal, '%s_bal_cv.joblib' % DOE)\n",
    "\n",
    "# print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fs_false_all_nwords_5000_mindf_1\n",
      "Original n_true, n_false: 2329 18717\n",
      "setting n_false_sample to:  18717\n",
      "Resampled n_true, n_false: 2329 18717\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 1:\n",
    "\n",
    "    X_train,Y_train,X_test, Y_test,X_valid, Y_valid= ns.load_data()\n",
    "    n_false_sample= None\n",
    "    n_features=5000\n",
    "\n",
    "    DOE='fs_false_all_nwords_%s_mindf_1' % (n_features)\n",
    "    print(DOE)\n",
    "\n",
    "    data_set2, bow_bal2, tfidf2 = ns.train_bow(X_train,Y_train, X_test,Y_test,n_false_sample ,  max_df=0.9,min_df=1, max_features=n_features )\n",
    "\n",
    "    with open('%s_data.pkl' % DOE, 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(data_set2, f)\n",
    "        \n",
    "        from joblib import dump, load\n",
    "        dump(tfidf2, '%s_tfidf.joblib' % DOE)\n",
    "        dump(bow_bal2, '%s_bal_cv.joblib' % DOE)\n",
    "        \n",
    "    print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=SelectKBest(chi2, k=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<21046x100 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 24421 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.fit_transform(data_set2['Xtrain'], data_set2['Ytrain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup=fs.get_support()\n",
    "words_all=np.array(bow_bal2.get_feature_names())\n",
    "words_fs=set(words_all[sup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common words in FS and in balanced:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array({'aid', 'hunger', 'item', 'bring', 'drought', 'donat', 'medic', 'wfp', 'distribut', 'help', 'medicin', 'product', 'peopl', 'inform', 'cloth', 'water', 'sleep', 'die', 'street', 'need', 'eat', 'hungri', 'tent', 'hygien', 'shelter', 'suppli', 'haiti', 'rice', 'earthquak', 'babi', 'blanket', 'food'},\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"common words in FS and in balanced:\\n\")\n",
    "\n",
    "common_words=np.array( words_bal.intersection(words_fs) )\n",
    "display(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in FS, not in balanced:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array({'potabl', 'sandwich', 'nt', 'brach', 'feuill', 'hurri', 'intersect', 'drinkabl', 'tonn', 'thirst', 'meal', 'clercin', 'famin', 'thomazeau', 'starv', 'clean', 'flour', 'eaten', 'jacket', 'kachipul', 'pack', 'impass', 'noodl', 'toy', 'biscuit', 'thirsti', 'oil', 'parcel', 'canapevert', 'bread', 'arcahai', 'feed', 'etc', 'nutriti', 'bean', 'insecur', 'jacmel', 'non', 'bag', 'rue', 'shortag', 'ton', 'leogan', 'instant', 'carrefour', 'nourish', 'box', 'hot', 'kilo', 'nonperish', 'will', 'avenu', 'nutrit', 'champagn', 'note', 'ration', 'packet', 'section', 'deliv', 'fort', 'toiletri', 'frere', 'gressier', 'laboul', 'perish', 'cook', 'can', 'delma'},\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"words in FS, not in balanced:\\n\")\n",
    "display( np.array(words_fs - words_bal) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in balanced, not in FS:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array({'affect', 'year', 'like', 'support', 'destroy', 'come', 'live', 'health', 'caus', 'school', 'region', 'heavi', 'work', 'week', 'home', 'day', 'famili', 'nation', 'commun', 'build', 'citi', 'storm', 'said', 'problem', 'power', 'emerg', 'provinc', 'good', 'messag', 'ask', 'govern', 'disast', 'countri', 'diseas', 'continu', 'children', 'area', 'district', 'flood', 'villag', 'includ', 'find', 'state', 'rain', 'assist', 'place', 'relief', 'sandi', 'world', 'unit', 'road', 'report', 'crop', 'time', 'hurrican', 'intern', 'thank', 'know', 'victim', 'receiv', 'provid', 'hous', 'local', 'damag', 'send', 'secur', 'want', 'kill'},\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"words in balanced, not in FS:\\n\")\n",
    "display( np.array(words_bal - words_fs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only_false_0_nwords_1000_mindf_1\n",
      "Original n_true, n_false: 2329 18717\n",
      "Resampled n_true, n_false: 2329 0\n"
     ]
    }
   ],
   "source": [
    "if 1:   \n",
    "    \n",
    "    X_train,Y_train,X_test, Y_test,X_valid, Y_valid= ns.load_data()\n",
    "    n_false_sample= 0\n",
    "    n_features=1000\n",
    "\n",
    "    DOE='only_false_%s_nwords_%s_mindf_1' % (n_false_sample,n_features)\n",
    "    print(DOE)\n",
    "\n",
    "    data_set3, bow_bal3, tfidf = ns.train_bow(X_train,Y_train, X_test,Y_test,n_false_sample ,  max_df=0.9,min_df=1, max_features=n_features )\n",
    "\n",
    "    with open('%s_data.pkl' % DOE, 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(data_set, f)\n",
    "        \n",
    "        from joblib import dump, load\n",
    "        dump(tfidf, '%s_tfidf.joblib' % DOE)\n",
    "        dump(bow_bal3, '%s_bal_cv.joblib' % DOE)\n",
    "        \n",
    "    print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=SelectKBest(chi2, k=100)\n",
    "fs.fit_transform(data_set3['Xtrain'], data_set3['Ytrain'])\n",
    "\n",
    "sup=fs.get_support()\n",
    "words_all2=np.array(bow_bal3.get_feature_names())\n",
    "words_fs2=set(words_all2[sup])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
