{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import random as rd\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.util import minibatch, compounding\n",
    "from nltk.stem.porter import *  \n",
    "\n",
    "import re\n",
    "import urllib.request\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_columns = None\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "from spacy import displacy\n",
    "import df_helper as dfh\n",
    "\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, validation_curve\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, fbeta_score, classification_report, confusion_matrix\n",
    "\n",
    "import sklearn.metrics as met\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, make_scorer, average_precision_score, auc, \\\n",
    "    accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, classification_report, \\\n",
    "    brier_score_loss, roc_auc_score\n",
    "\n",
    "from scipy.stats import randint as sp_randint , uniform\n",
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentence(bow, message):\n",
    "    # for eli5\n",
    "    \n",
    "    print(\"original: \",message)\n",
    "    \n",
    "    words=np.array(bow.get_feature_names())\n",
    "    TR=bow.transform([ message ])\n",
    "    g,ind=TR.nonzero()\n",
    "    transformed=\",\".join(words[ind]) \n",
    "    print(\"transformed: \",transformed)\n",
    "    print()\n",
    "    \n",
    "def explain_message(pos,fn_messages,pipe,dataset,top=10):\n",
    "    \"\"\"\n",
    "    explain_message(pos,X_test[y_fn],pipe,bow1k_bal)\n",
    "    \"\"\"\n",
    "    \n",
    "    message=fn_messages.loc[pos]\n",
    "    transform_sentence(pipe.steps[0][1],message)\n",
    "\n",
    "    print('Predicted class:', pipe.predict([message ])[0] )\n",
    "    display(eli5.show_prediction(pipe.steps[2][1],   dataset['Xtest'][pos,:] , target_names=[0,1],\n",
    "                         feature_names= pipe.steps[0][1].get_feature_names(),top=top) )\n",
    "    \n",
    "    \n",
    "def spacy_tokenizer_stemmer(message):\n",
    "    message=re.sub(\"[1-9#@$'!*+%\\\".()!,]?;\",'',message).replace('','').replace('-','')\n",
    "    message=' '.join(message.split())\n",
    "    doc=nlp(message)\n",
    "    words=[]\n",
    "\n",
    "    stemmer = PorterStemmer()  \n",
    "    \n",
    "    remove_ent=[]\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['GPE','LOC','NORP','FAC','ORG','LANGUAGE']:\n",
    "            remove_ent.append(ent.text)\n",
    "\n",
    "    # remove punctuation etc\n",
    "    for token in doc:\n",
    "        if ( (~token.is_stop)   & (token.pos_!='NUM') & (token.pos_!='PUNCT') & (token.pos_!='SYM') &\n",
    "           ~(token.text in (remove_ent)) & (len(token.text)>1) ):\n",
    "            words.append( stemmer.stem(token.text) )\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('disaster_response_messages_training.csv')\n",
    "df=df[['message','food']]\n",
    "\n",
    "test_df=pd.read_csv('disaster_response_messages_test.csv')\n",
    "test_df=test_df[['message','food']]\n",
    "\n",
    "valid_df=pd.read_csv('disaster_response_messages_validation.csv')\n",
    "valid_df=valid_df[['message','food']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def imbalanced_undersample(X_train,Y_train,n_false_sample, classes=[False,True]):\n",
    "    \"\"\"\n",
    "    resamples the false class classes[0] with n_false_sample to correct for imbalanced data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_false=Y_train.loc[Y_train==classes[0]].shape[0]\n",
    "    n_true=Y_train.loc[Y_train==classes[1]].shape[0]\n",
    "    ind_false=Y_train.loc[Y_train==classes[0]].index\n",
    "    ind_true=Y_train.loc[Y_train==classes[1]].index\n",
    "\n",
    "    print('Original n_true, n_false:' ,n_true,n_false)\n",
    "    \n",
    "    ind_s=sample_without_replacement(n_false,n_false_sample)\n",
    "    ind=np.hstack((ind_true,ind_false[ind_s]))\n",
    "    np.random.shuffle(ind)\n",
    "\n",
    "    X_train2=X_train.iloc[ind].copy()\n",
    "    Y_train2=Y_train.iloc[ind].copy()\n",
    "    \n",
    "    n_false2=Y_train2.loc[Y_train==classes[0]].shape[0]\n",
    "    n_true2=Y_train2.loc[Y_train==classes[1]].shape[0]\n",
    "    \n",
    "    print('Resampled n_true, n_false:' ,n_true2,n_false2)\n",
    "    \n",
    "    return(X_train2,Y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bow(X_train,Y_train, X_test,Y_test,n_false_sample ,  max_df=0.9,min_df=1, max_features=2000 ):\n",
    "    \"\"\"\n",
    "    transforms the training and test set with a reduced B.O.W. with only n_false_sample \n",
    "    usage:\n",
    "    \n",
    "    bow1k_bal, bow_bal, tfidf = train_bow(X_train,Y_train, X_test,Y_test,n_false_sample ,  max_df=0.9,min_df=1, max_features=2000 )\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    X_train2,Y_train2=imbalanced_undersample(X_train,Y_train,n_false_sample, classes=[False,True])\n",
    "\n",
    "    bow_bal=CountVectorizer(tokenizer = spacy_tokenizer_stemmer, max_df=max_df,min_df=min_df, max_features=max_features)\n",
    "    tfidf=TfidfTransformer()     \n",
    "\n",
    "    bow_bal.fit(X_train2)\n",
    "    Xbow_train=bow_bal.transform(X_train)\n",
    "    X_train_tdidf2 = tfidf.fit_transform(Xbow_train)\n",
    "\n",
    "    Xbow_test = bow_bal.transform(X_test)\n",
    "    X_test_tdidf2 = tfidf.transform(Xbow_test)\n",
    "    \n",
    "    bow1k_bal={'Xtrain':X_train_tdidf2, 'Ytrain':Y_train, 'Xtest':X_test_tdidf2, 'Ytest':Y_test}\n",
    "    return( bow1k_bal, bow_bal, tfidf )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_est=50\n",
    "clf= RandomForestClassifier(  criterion=\"entropy\",class_weight=\"balanced\", n_estimators=N_est)\n",
    "\n",
    "dfh.plot_learning_curve(clf, 'RandomForest N=%d' % N_est, bow1k_bal['Xtrain'], bow1k_bal['Ytrain'], ylim=None, cv=5, \n",
    "                    n_jobs=4, train_sizes=np.linspace(.1, 1.0, 5), scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(bow1k_bal['Xtrain'], bow1k_bal['Ytrain'])\n",
    "y_score = clf.predict_proba(bow1k_bal['Xtest'] )[:,1]\n",
    "y_pred = clf.predict(bow1k_bal['Xtest'])\n",
    "Y_test=bow1k_bal['Ytest']\n",
    "rf=dfh.add_metrics(Y_test,y_score,y_pred,'RF_50_bal',rf)\n",
    "\n",
    "print(\"accuracy_score: \",accuracy_score(Y_test,y_pred))\n",
    "print(\"average precision: \",average_precision_score(Y_test,y_score))\n",
    "\n",
    "print(\"balanced_accuracy_score: \",balanced_accuracy_score(Y_test,y_pred))\n",
    "print(\"ROC AUC: \",roc_auc_score(Y_test,y_score))\n",
    "print()\n",
    "print(classification_report(Y_test,y_pred))\n",
    "\n",
    "dfh.print_cm(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipe_bal= Pipeline([\n",
    "    ('bow',bow_bal),\n",
    "    ('tfidf', tfidf    ),\n",
    "    ('clf',clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eli5.lime import TextExplainer\n",
    "\n",
    "# te = TextExplainer(random_state=42);\n",
    "# te.fit(message, full_pipe_bal.predict_proba)\n",
    "# te.show_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr=LogisticRegression(random_state=0, solver='liblinear',penalty='l1',max_iter=200,class_weight='balanced' )\n",
    "dfh.plot_learning_curve(clf_lr, 'LogisticRegression',  bow1k_bal['Xtrain'], bow1k_bal['Ytrain'], ylim=None, cv=5, \n",
    "                    n_jobs=4, train_sizes=np.linspace(.1, 1.0, 5), scoring='f1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
